{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Decision_Tree_SWAT.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"gSoJfne5yQF_","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import random\n","from pprint import pprint"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jvjNc8JODGo9","colab_type":"text"},"source":["# Load and Prepare Data"]},{"cell_type":"markdown","metadata":{"id":"05ErqN5ZDGo_","colab_type":"text"},"source":["#### Format of the data\n","- last column of the data frame must contain the label and it must also be called \"label\"\n","- there should be no missing values in the data frame"]},{"cell_type":"code","metadata":{"id":"59OFB43eDGpG","colab_type":"code","colab":{}},"source":["df = pd.read_csv(\"/content/train_modified.csv\")\n","# df = df.drop(\"Id\", axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iY1AMjlvDGpN","colab_type":"code","outputId":"6b8cca1d-34c8-4d76-f387-8bbee578274a","executionInfo":{"status":"ok","timestamp":1574712197320,"user_tz":300,"elapsed":544,"user":{"displayName":"Saket Chaudhary","photoUrl":"","userId":"05418537525989083255"}},"colab":{"base_uri":"https://localhost:8080/","height":395}},"source":["df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Timestamp</th>\n","      <th>FIT101</th>\n","      <th>LIT101</th>\n","      <th>MV101</th>\n","      <th>P101</th>\n","      <th>P102</th>\n","      <th>AIT201</th>\n","      <th>AIT202</th>\n","      <th>AIT203</th>\n","      <th>FIT201</th>\n","      <th>MV201</th>\n","      <th>P201</th>\n","      <th>P202</th>\n","      <th>P203</th>\n","      <th>P204</th>\n","      <th>P205</th>\n","      <th>P206</th>\n","      <th>DPIT301</th>\n","      <th>FIT301</th>\n","      <th>LIT301</th>\n","      <th>MV301</th>\n","      <th>MV302</th>\n","      <th>MV303</th>\n","      <th>MV304</th>\n","      <th>P301</th>\n","      <th>P302</th>\n","      <th>AIT401</th>\n","      <th>AIT402</th>\n","      <th>FIT401</th>\n","      <th>LIT401</th>\n","      <th>P401</th>\n","      <th>P402</th>\n","      <th>P403</th>\n","      <th>P404</th>\n","      <th>UV401</th>\n","      <th>AIT501</th>\n","      <th>AIT502</th>\n","      <th>AIT503</th>\n","      <th>AIT504</th>\n","      <th>FIT501</th>\n","      <th>FIT502</th>\n","      <th>FIT503</th>\n","      <th>FIT504</th>\n","      <th>P501</th>\n","      <th>P502</th>\n","      <th>PIT501</th>\n","      <th>PIT502</th>\n","      <th>PIT503</th>\n","      <th>FIT601</th>\n","      <th>P601</th>\n","      <th>P602</th>\n","      <th>P603</th>\n","      <th>Normal/Attack</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>22/12/2015 4:00:00 PM</td>\n","      <td>2.470294</td>\n","      <td>261.5804</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>244.3284</td>\n","      <td>8.19008</td>\n","      <td>306.101</td>\n","      <td>2.471278</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>20.79839</td>\n","      <td>2.235275</td>\n","      <td>327.4401</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>171.0587</td>\n","      <td>0.0</td>\n","      <td>238.0544</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>7.308575</td>\n","      <td>176.8265</td>\n","      <td>263.6504</td>\n","      <td>12.68905</td>\n","      <td>0.001666</td>\n","      <td>0.001409</td>\n","      <td>0.001664</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>10.02948</td>\n","      <td>0.0</td>\n","      <td>4.277749</td>\n","      <td>0.000256</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>22/12/2015 4:00:01 PM</td>\n","      <td>2.457163</td>\n","      <td>261.1879</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>244.3284</td>\n","      <td>8.19008</td>\n","      <td>306.101</td>\n","      <td>2.468587</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>20.79839</td>\n","      <td>2.234507</td>\n","      <td>327.4401</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>171.0587</td>\n","      <td>0.0</td>\n","      <td>238.1312</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>7.308575</td>\n","      <td>176.8008</td>\n","      <td>263.6504</td>\n","      <td>12.68905</td>\n","      <td>0.001666</td>\n","      <td>0.001409</td>\n","      <td>0.001664</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>10.02948</td>\n","      <td>0.0</td>\n","      <td>4.277749</td>\n","      <td>0.000256</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>22/12/2015 4:00:02 PM</td>\n","      <td>2.439548</td>\n","      <td>260.9131</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>244.3284</td>\n","      <td>8.19008</td>\n","      <td>306.101</td>\n","      <td>2.467305</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>20.84320</td>\n","      <td>2.233354</td>\n","      <td>327.4401</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>171.0587</td>\n","      <td>0.0</td>\n","      <td>238.0544</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>7.308575</td>\n","      <td>176.8008</td>\n","      <td>263.6504</td>\n","      <td>12.68905</td>\n","      <td>0.001666</td>\n","      <td>0.001409</td>\n","      <td>0.001664</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>10.02948</td>\n","      <td>0.0</td>\n","      <td>4.277749</td>\n","      <td>0.000256</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>22/12/2015 4:00:03 PM</td>\n","      <td>2.428338</td>\n","      <td>260.2850</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>244.3284</td>\n","      <td>8.19008</td>\n","      <td>306.101</td>\n","      <td>2.466536</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>20.84320</td>\n","      <td>2.233354</td>\n","      <td>327.2799</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>171.0587</td>\n","      <td>0.0</td>\n","      <td>238.2081</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>7.308575</td>\n","      <td>176.8008</td>\n","      <td>263.6504</td>\n","      <td>12.68905</td>\n","      <td>0.001666</td>\n","      <td>0.001409</td>\n","      <td>0.001664</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>10.02948</td>\n","      <td>0.0</td>\n","      <td>4.277749</td>\n","      <td>0.000256</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>22/12/2015 4:00:04 PM</td>\n","      <td>2.424815</td>\n","      <td>259.8925</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>244.4245</td>\n","      <td>8.19008</td>\n","      <td>306.101</td>\n","      <td>2.466536</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>20.84320</td>\n","      <td>2.233354</td>\n","      <td>327.1597</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>171.0587</td>\n","      <td>0.0</td>\n","      <td>238.4389</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>7.308575</td>\n","      <td>176.8008</td>\n","      <td>263.6504</td>\n","      <td>12.68905</td>\n","      <td>0.001666</td>\n","      <td>0.001409</td>\n","      <td>0.001664</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>10.02948</td>\n","      <td>0.0</td>\n","      <td>4.277749</td>\n","      <td>0.000256</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                Timestamp    FIT101    LIT101  ...  P602  P603  Normal/Attack\n","0   22/12/2015 4:00:00 PM  2.470294  261.5804  ...   1.0   1.0         Normal\n","1   22/12/2015 4:00:01 PM  2.457163  261.1879  ...   1.0   1.0         Normal\n","2   22/12/2015 4:00:02 PM  2.439548  260.9131  ...   1.0   1.0         Normal\n","3   22/12/2015 4:00:03 PM  2.428338  260.2850  ...   1.0   1.0         Normal\n","4   22/12/2015 4:00:04 PM  2.424815  259.8925  ...   1.0   1.0         Normal\n","\n","[5 rows x 53 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"Aw2nfgNQDGpb","colab_type":"text"},"source":["# Train-Test-Split"]},{"cell_type":"code","metadata":{"id":"VQaaddokKItA","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from pprint import pprint\n","dataset = pd.read_csv('/content/train_modified.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xF-22ZNzKhU5","colab_type":"code","colab":{}},"source":["def cal_entropy(target_col):\n","    elements,counts = np.unique(target_col,return_counts = True)\n","    cal_entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n","    return cal_entropy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BX27I-2PLA9r","colab_type":"code","colab":{}},"source":["def InfoGain(data,split_attribute_name,target_name=\"class\"):\n","    \"\"\"\n","    Calculate the information gain of a dataset. This function takes three parameters:\n","    1. data = The dataset for whose feature the IG should be calculated\n","    2. split_attribute_name = the name of the feature for which the information gain should be calculated\n","    3. target_name = the name of the target feature. The default for this example is \"class\"\n","    \"\"\"    \n","    #Calculate the entropy of the total dataset\n","    total_entropy = entropy(data[target_name])\n","    \n","    ##Calculate the entropy of the dataset\n","    \n","    #Calculate the values and the corresponding counts for the split attribute \n","    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n","    \n","    #Calculate the weighted entropy\n","    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n","    \n","    #Calculate the information gain\n","    Information_Gain = total_entropy - Weighted_Entropy\n","    return Information_Gain"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"40tMNkHaDGpc","colab_type":"code","outputId":"aa3bf33a-9d0e-4a3e-fb92-b17f9e5c3c8a","executionInfo":{"status":"error","timestamp":1574712098090,"user_tz":300,"elapsed":1669,"user":{"displayName":"Saket Chaudhary","photoUrl":"","userId":"05418537525989083255"}},"colab":{"base_uri":"https://localhost:8080/","height":381}},"source":["def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None):\n","    \"\"\"\n","    ID3 Algorithm: This function takes five paramters:\n","    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset\n"," \n","    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset\n","    in the case the dataset delivered by the first parameter is empty\n","    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process\n","    we have to remove features from our dataset --> Splitting at each node\n","    4. target_attribute_name = the name of the target attribute\n","    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is \n","    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature\n","    space, we want to return the mode target feature value of the direct parent node.\n","    \"\"\"   \n","    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n","    \n","    #If all target_values have the same value, return this value\n","    if len(np.unique(data[target_attribute_name])) <= 1:\n","        return np.unique(data[target_attribute_name])[0]\n","    \n","    #If the dataset is empty, return the mode target feature value in the original dataset\n","    elif len(data)==0:\n","        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n","    \n","    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n","    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n","    #the mode target feature value is stored in the parent_node_class variable.\n","    \n","    elif len(features) ==0:\n","        return parent_node_class\n","    \n","    #If none of the above holds true, grow the tree!\n","    \n","    else:\n","        #Set the default value for this node --> The mode target feature value of the current node\n","        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n","        \n","        #Select the feature which best splits the dataset\n","        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n","        best_feature_index = np.argmax(item_values)\n","        best_feature = features[best_feature_index]\n","        \n","        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n","        #gain in the first run\n","        tree = {best_feature:{}}\n","        \n","        \n","        #Remove the feature with the best inforamtion gain from the feature space\n","        features = [i for i in features if i != best_feature]\n","        \n","        #Grow a branch under the root node for each possible value of the root node feature\n","        \n","        for value in np.unique(data[best_feature]):\n","            value = value\n","            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n","            sub_data = data.where(data[best_feature] == value).dropna()\n","            \n","            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n","            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n","            \n","            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n","            tree[best_feature][value] = subtree\n","            \n","        return(tree)    \n","                \n","###################\n","###################\n","    \n","    \n","def predict(query,tree,default = 1):\n","    \"\"\"\n","    Prediction of a new/unseen query instance. This takes two parameters:\n","    1. The query instance as a dictionary of the shape {\"feature_name\":feature_value,...}\n","    2. The tree \n","    We do this also in a recursive manner. That is, we wander down the tree and check if we have reached a leaf or if we are still in a sub tree. \n","    Since this is a important step to understand, the single steps are extensively commented below.\n","    1.Check for every feature in the query instance if this feature is existing in the tree.keys() for the first call, \n","    tree.keys() only contains the value for the root node \n","    --> if this value is not existing, we can not make a prediction and have to \n","    return the default value which is the majority value of the target feature\n","    2. First of all we have to take care of a important fact: Since we train our model with a database A and then show our model\n","    a unseen query it may happen that the feature values of these query are not existing in our tree model because non of the\n","    training instances has had such a value for this specific feature. \n","    For instance imagine the situation where your model has only seen animals with one to four\n","    legs - The \"legs\" node in your model will only have four outgoing branches (from one to four). If you now show your model\n","    a new instance (animal) which has for the legs feature the vale 5, you have to tell your model what to do in such a \n","    situation because otherwise there is no classification possible because in the classification step you try to \n","    run down the outgoing branch with the value 5 but there is no such a branch. Hence: Error and no Classification!\n","    We can address this issue with a classification value of for instance (999) which tells us that there is no classification\n","    possible or we assign the most frequent target feature value of our dataset used to train the model. Or, in for instance \n","    medical application we can return the most worse case - just to make sure... \n","    We can also return the most frequent value of the direct parent node. To make a long story short, we have to tell the model \n","    what to do in this situation.\n","    In our example, since we are dealing with animal species where a false classification is not that critical, we will assign\n","    the value 1 which is the value for the mammal species (for convenience).\n","    3. Address the key in the tree which fits the value for key --> Note that key == the features in the query. \n","    Because we want the tree to predict the value which is hidden under the key value (imagine you have a drawn tree model on \n","    the table in front of you and you have a query instance for which you want to predict the target feature \n","    - What are you doing? - Correct:\n","    You start at the root node and wander down the tree comparing your query to the node values. Hence you want to have the\n","    value which is hidden under the current node. If this is a leaf, perfect, otherwise you wander the tree deeper until you\n","    get to a leaf node. \n","    Though, you want to have this \"something\" [either leaf or sub_tree] which is hidden under the current node\n","    and hence we must address the node in the tree which == the key value from our query instance. \n","    This is done with tree[keys]. Next you want to run down the branch of this node which is equal to the value given \"behind\"\n","    the key value of your query instance e.g. if you find \"legs\" == to tree.keys() that is, for the first run == the root node.\n","    You want to run deeper and therefore you have to address the branch at your node whose value is == to the value behind key.\n","    This is done with query[key] e.g. query[key] == query['legs'] == 0 --> Therewith we run down the branch of the node with the\n","    value 0. Summarized, in this step we want to address the node which is hidden behind a specific branch of the root node (in the first run)\n","    this is done with: result = [key][query[key]]\n","    4. As said in the 2. step, we run down the tree along nodes and branches until we get to a leaf node.\n","    That is, if result = tree[key][query[key]] returns another tree object (we have represented this by a dict object --> \n","    that is if result is a dict object) we know that we have not arrived at a root node and have to run deeper the tree. \n","    Okay... Look at your drawn tree in front of you... what are you doing?...well, you run down the next branch... \n","    exactly as we have done it above with the slight difference that we already have passed a node and therewith \n","    have to run only a fraction of the tree --> You clever guy! That \"fraction of the tree\" is exactly what we have stored\n","    under 'result'.\n","    So we simply call our predict method using the same query instance (we do not have to drop any features from the query\n","    instance since for instance the feature for the root node will not be available in any of the deeper sub_trees and hence \n","    we will simply not find that feature) as well as the \"reduced / sub_tree\" stored in result.\n","    SUMMARIZED: If we have a query instance consisting of values for features, we take this features and check if the \n","    name of the root node is equal to one of the query features.\n","    If this is true, we run down the root node outgoing branch whose value equals the value of query feature == the root node.\n","    If we find at the end of this branch a leaf node (not a dict object) we return this value (this is our prediction).\n","    If we instead find another node (== sub_tree == dict objct) we search in our query for the feature which equals the value \n","    of that node. Next we look up the value of our query feature and run down the branch whose value is equal to the \n","    query[key] == query feature value. And as you can see this is exactly the recursion we talked about\n","    with the important fact that for each node we run down the tree, we check only the nodes and branches which are \n","    below this node and do not run the whole tree beginning at the root node \n","    --> This is why we re-call the classification function with 'result'\n","    \"\"\"\n","    \n","    \n","    #1.\n","    for key in list(query.keys()):\n","        if key in list(tree.keys()):\n","            #2.\n","            try:\n","                result = tree[key][query[key]] \n","            except:\n","                return default\n","  \n","            #3.\n","            result = tree[key][query[key]]\n","            #4.\n","            if isinstance(result,dict):\n","                return predict(query,result)\n","            else:\n","                return result\n","        \n","        \n","\"\"\"\n","Check the accuracy of our prediction.\n","The train_test_split function takes the dataset as parameter which should be divided into\n","a training and a testing set. The test function takes two parameters, which are the testing data as well as the tree model.\n","\"\"\"\n","###################\n","###################\n","def train_test_split(dataset):\n","    training_data = dataset.iloc[:80].reset_index(drop=True)#We drop the index respectively relabel the index\n","    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n","    testing_data = dataset.iloc[80:].reset_index(drop=True)\n","    return training_data,testing_data\n","training_data = train_test_split(dataset)[0]\n","testing_data = train_test_split(dataset)[1] \n","def test(data,tree):\n","    #Create new query instances by simply removing the target feature column from the original dataset and \n","    #convert it to a dictionary\n","    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n","    \n","    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n","    predicted = pd.DataFrame(columns=[\"predicted\"]) \n","    \n","    #Calculate the prediction accuracy\n","    for i in range(len(data)):\n","        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n","    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')\n","    \n","\"\"\"\n","Train the tree, Print the tree and predict the accuracy\n","\"\"\"\n","tree = ID3(training_data,training_data,training_data.columns[:-1])\n","pprint(tree)\n","test(testing_data,tree)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-9c96faf16ac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train_modified.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Import all columns omitting the fist which consists the names of the animals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#We drop the animal names since this is not a good feature to split the data on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'animal_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4115\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4117\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4118\u001b[0m         )\n\u001b[1;32m   4119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3912\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3913\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3914\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3944\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3945\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3946\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3947\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5340\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found in axis\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5341\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['animal_name'] not found in axis\""]}]},{"cell_type":"code","metadata":{"id":"w970peLiDGpg","colab_type":"code","colab":{}},"source":["random.seed(0)\n","train_df, test_df = train_test_split(df, test_size=20)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1T9cohN-CEmI","colab_type":"code","colab":{}},"source":["kf=train_df.loc[:,'MV201':'P206'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEK-XBAtCgmD","colab_type":"code","outputId":"30b359e2-e41b-409e-abb6-dccbce6337a4","executionInfo":{"status":"ok","timestamp":1573747859210,"user_tz":300,"elapsed":558,"user":{"displayName":"Saket Chaudhary","photoUrl":"","userId":"05418537525989083255"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["kf"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 1, 1, ..., 1, 2, 1],\n","       [2, 1, 1, ..., 1, 2, 1],\n","       [2, 1, 1, ..., 1, 2, 1],\n","       ...,\n","       [2, 1, 1, ..., 1, 2, 1],\n","       [2, 1, 1, ..., 1, 2, 1],\n","       [2, 1, 1, ..., 1, 2, 1]])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"VSYWaCQbBxp4","colab_type":"code","colab":{}},"source":["unique_classes, counts_unique_classes = np.unique(kf, return_counts=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A_zh-WkeCMOq","colab_type":"code","outputId":"6a9c8d30-de11-407c-9222-9bedf8934ffd","executionInfo":{"status":"ok","timestamp":1573133087310,"user_tz":300,"elapsed":383,"user":{"displayName":"Saket Chaudhary","photoUrl":"","userId":"05418537525989083255"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["unique_classes, counts_unique_classes"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([1, 2]), array([2406392, 1071068]))"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"ngnSMusiCTYD","colab_type":"code","outputId":"afda1ea6-ba06-420d-a21b-04ae16766300","executionInfo":{"status":"ok","timestamp":1573133048155,"user_tz":300,"elapsed":367,"user":{"displayName":"Saket Chaudhary","photoUrl":"","userId":"05418537525989083255"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(counts_unique_classes)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[2406392 1071068]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jYsM1TnTDGp1","colab_type":"text"},"source":["# Helper Functions"]},{"cell_type":"code","metadata":{"id":"gW82rdJrDGp2","colab_type":"code","colab":{}},"source":["data = train_df.loc[:,\"FIT101\":\"P603\"].values\n","data[:5]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4NuS1bzk_Vfv","colab_type":"code","outputId":"a49275bb-3e39-421d-c51d-83ea1ab3cc30","executionInfo":{"status":"ok","timestamp":1573132489567,"user_tz":300,"elapsed":374,"user":{"displayName":"Saket Chaudhary","photoUrl":"","userId":"05418537525989083255"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(data[0])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["51"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"J6KmBAhwIqPM","colab_type":"text"},"source":["#Probability"]},{"cell_type":"code","metadata":{"id":"rAdo5qksIpy4","colab_type":"code","colab":{}},"source":["def gini(data):\n","  label_column = data[:-1]\n","  unique_classes=np.unique(label_column)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uDafIzMIDGp7","colab_type":"text"},"source":["### Data pure?"]},{"cell_type":"code","metadata":{"id":"H7FJqEGBDGp-","colab_type":"code","colab":{}},"source":["def check_purity(data):\n","    \n","    label_column = data[:-1]\n","    unique_classes = np.unique(label_column)\n","\n","    if len(unique_classes) == 1:\n","        return True\n","    else:\n","        return False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WzO4L-03DGqD","colab_type":"text"},"source":["### Classify"]},{"cell_type":"code","metadata":{"id":"6mSNUpRlDGqF","colab_type":"code","colab":{}},"source":["def classify_data(data):\n","    \n","    label_column = data[:,-1]\n","    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n","    print(np.asarray((unique_classes, counts_unique_classes)))\n","\n","    index = counts_unique_classes.argmax()\n","    print(index)\n","    classification = unique_classes[index]\n","    \n","    return classification"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUF2Pda9E2Ot","colab_type":"code","outputId":"7e20d050-bd2c-4aef-db21-cb9759e20133","executionInfo":{"status":"ok","timestamp":1573133709337,"user_tz":300,"elapsed":333,"user":{"displayName":"Saket Chaudhary","photoUrl":"","userId":"05418537525989083255"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(496780, 51)"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"WrMz3nqwEpj6","colab_type":"code","colab":{}},"source":["for i in range(data.shape[1]):\n","  unique, counts = np.unique(data[:,i], return_counts=True)\n","  k=dict(zip(unique, counts))\n","  print( k)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gU5Gra5O6wU4","colab_type":"code","outputId":"8e1fa613-d092-4615-b03a-a92b883134f8","executionInfo":{"status":"ok","timestamp":1573133643239,"user_tz":300,"elapsed":423,"user":{"displayName":"Saket Chaudhary","photoUrl":"","userId":"05418537525989083255"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["np.unique(kf[:,1])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1])"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"JMs42IpYDGqT","colab_type":"text"},"source":["### Potential splits?"]},{"cell_type":"code","metadata":{"id":"_BUIuD41DGqU","colab_type":"code","colab":{}},"source":["def get_potential_splits(data):\n","    \n","    potential_splits = {}\n","    _, n_columns = data.shape\n","    for column_index in range(n_columns - 1):          # excluding the last column which is the label\n","        values = data[:, column_index]\n","        unique_values = np.unique(values)\n","        \n","        type_of_feature = FEATURE_TYPES[column_index]\n","        if type_of_feature == \"continuous\":\n","            potential_splits[column_index] = []\n","            for index in range(len(unique_values)):\n","                if index != 0:\n","                    current_value = unique_values[index]\n","                    previous_value = unique_values[index - 1]\n","                    potential_split = (current_value + previous_value) / 2\n","\n","                    potential_splits[column_index].append(potential_split)\n","        \n","        # feature is categorical\n","        # (there need to be at least 2 unique values, otherwise in the\n","        # split_data function data_below would contain all data points\n","        # and data_above would be empty)\n","        elif len(unique_values) > 1:\n","            potential_splits[column_index] = unique_values\n","    \n","    return potential_splits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KIIdGjSWDPTu","colab_type":"code","outputId":"91f12b9f-f206-4848-ba85-bef477e84d37","executionInfo":{"status":"ok","timestamp":1573133306677,"user_tz":300,"elapsed":330,"user":{"displayName":"Saket Chaudhary","photoUrl":"","userId":"05418537525989083255"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["kf"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 1, 1, 2, 1, 2, 1])"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"ZbqhAG8IDFPa","colab_type":"code","outputId":"eef60992-caf3-4327-cf24-b5a22132d90d","executionInfo":{"status":"ok","timestamp":1573133256351,"user_tz":300,"elapsed":406,"user":{"displayName":"Saket Chaudhary","photoUrl":"","userId":"05418537525989083255"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["get_potential_splits(kf)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: [1.5], 1: [], 3: array([1, 2]), 5: [1.5]}"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"HbntPJ6CDGqb","colab_type":"text"},"source":["### Split Data"]},{"cell_type":"code","metadata":{"id":"SZ5_OszNDGqd","colab_type":"code","colab":{}},"source":["def split_data(data, split_column, split_value):\n","    \n","    split_column_values = data[:, split_column]\n","\n","    type_of_feature = FEATURE_TYPES[split_column]\n","    if type_of_feature == \"continuous\":\n","        data_below = data[split_column_values <= split_value]\n","        data_above = data[split_column_values >  split_value]\n","    \n","    # feature is categorical   \n","    else:\n","        data_below = data[split_column_values == split_value]\n","        data_above = data[split_column_values != split_value]\n","    \n","    return data_below, data_above"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JPOCt9VDGqj","colab_type":"text"},"source":["### Lowest Overall Entropy?"]},{"cell_type":"code","metadata":{"id":"2x_4dHluDGql","colab_type":"code","colab":{}},"source":["def calculate_entropy(data):\n","    \n","    label_column = data[:, -1]\n","    _, counts = np.unique(label_column, return_counts=True)\n","\n","    probabilities = counts / counts.sum()\n","    entropy = sum(probabilities * -np.log2(probabilities))\n","     \n","    return entropy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UXi9zfhiDGrM","colab_type":"text"},"source":["### Determine Type of Feature"]},{"cell_type":"code","metadata":{"id":"vdh940bpDGqv","colab_type":"code","colab":{}},"source":["def calculate_overall_entropy(data_below, data_above):\n","    \n","    n = len(data_below) + len(data_above)\n","    p_data_below = len(data_below) / n\n","    p_data_above = len(data_above) / n\n","\n","    overall_entropy =  (p_data_below * calculate_entropy(data_below) \n","                      + p_data_above * calculate_entropy(data_above))\n","    \n","    return overall_entropy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2HyA9EADGq1","colab_type":"code","colab":{}},"source":["def determine_best_split(data, potential_splits):\n","    \n","    overall_entropy = 9999\n","    for column_index in potential_splits:\n","        for value in potential_splits[column_index]:\n","            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n","            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n","\n","            if current_overall_entropy <= overall_entropy:\n","                overall_entropy = current_overall_entropy\n","                best_split_column = column_index\n","                best_split_value = value\n","    \n","    return best_split_column, best_split_value"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqreP3Y3DGq5","colab_type":"text"},"source":["# Decision Tree Algorithm"]},{"cell_type":"markdown","metadata":{"id":"eLaZxFylDGrA","colab_type":"text"},"source":["### Representation of the Decision Tree"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ya0mzY3w6ZYJ"},"source":["### Determine Type of Feature"]},{"cell_type":"code","metadata":{"id":"RBj_4VWiDGrO","colab_type":"code","colab":{}},"source":["def determine_type_of_feature(df):\n","    \n","    feature_types = []\n","    n_unique_values_treshold = 15\n","    for feature in df.columns:\n","        if feature != \"label\":\n","            unique_values = df[feature].unique()\n","            example_value = unique_values[0]\n","\n","            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n","                feature_types.append(\"categorical\")\n","            else:\n","                feature_types.append(\"continuous\")\n","    \n","    return feature_types"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RuKrnmAxDGrg","colab_type":"text"},"source":["### Algorithm"]},{"cell_type":"code","metadata":{"id":"MIKMU647DGri","colab_type":"code","colab":{}},"source":["def decision_tree_algorithm(df, counter=0, min_samples=2, max_depth=5):\n","    \n","    # data preparations\n","    if counter == 0:\n","        global COLUMN_HEADERS, FEATURE_TYPES\n","        COLUMN_HEADERS = df.columns\n","        FEATURE_TYPES = determine_type_of_feature(df)\n","        data = df.values\n","    else:\n","        data = df           \n","    \n","    \n","    # base cases\n","    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n","        classification = classify_data(data)\n","        \n","        return classification\n","\n","    \n","    # recursive part\n","    else:    \n","        counter += 1\n","\n","        # helper functions \n","        potential_splits = get_potential_splits(data)\n","        split_column, split_value = determine_best_split(data, potential_splits)\n","        data_below, data_above = split_data(data, split_column, split_value)\n","        \n","        # determine question\n","        feature_name = COLUMN_HEADERS[split_column]\n","        type_of_feature = FEATURE_TYPES[split_column]\n","        if type_of_feature == \"continuous\":\n","            question = \"{} <= {}\".format(feature_name, split_value)\n","            \n","        # feature is categorical\n","        else:\n","            question = \"{} = {}\".format(feature_name, split_value)\n","        \n","        # instantiate sub-tree\n","        sub_tree = {question: []}\n","        \n","        # find answers (recursion)\n","        yes_answer = decision_tree_algorithm(data_below, counter, min_samples, max_depth)\n","        no_answer = decision_tree_algorithm(data_above, counter, min_samples, max_depth)\n","        \n","        # If the answers are the same, then there is no point in asking the qestion.\n","        # This could happen when the data is classified even though it is not pure\n","        # yet (min_samples or max_depth base case).\n","        if yes_answer == no_answer:\n","            sub_tree = yes_answer\n","        else:\n","            sub_tree[question].append(yes_answer)\n","            sub_tree[question].append(no_answer)\n","        \n","        return sub_tree"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5WBFPGvgAtIV","colab_type":"code","colab":{}},"source":["k_train=train_df.loc[:,\"FIT101\":\"P603\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0yfdc22xDGrs","colab_type":"code","colab":{}},"source":["tree = decision_tree_algorithm(k_train, max_depth=3)\n","pprint(tree)"],"execution_count":0,"outputs":[]}]}